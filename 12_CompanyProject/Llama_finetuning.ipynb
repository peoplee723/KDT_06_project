{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\KDP-25\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토큰 분류 모델 설계 과정\n",
    "\n",
    "*   데이터 전처리\n",
    "*   학습 파라미터 설정\n",
    "*   학습 및 평가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터 전처리<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "import pandas as pd\n",
    "import json, os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 불러오기\n",
    "test_list=os.listdir('./character_test/')\n",
    "for test in test_list:\n",
    "    if test_list.index(test)==0:\n",
    "        testingDF=pd.read_json(f'./character_test/{test}', encoding='utf-8')\n",
    "    else:\n",
    "        testingDF2=pd.read_json(f'./character_test/{test}', encoding='utf-8')\n",
    "        testingDF=pd.concat([testingDF, testingDF2])\n",
    "testingDF.to_csv('./testDF.csv', index=False)\n",
    "\n",
    "#새 통합 데이터 부르기\n",
    "tagDF=pd.read_json('./data/raw_data/learning_sentence.json')\n",
    "text=pd.read_table('./data/raw_data/learning_sentence.txt')\n",
    "tagDF['sentence']=text['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[오, 프, 커, 피, 에, 서, 아, 메, 리, 카, 노, 핫, 으, 로, 2, ...</td>\n",
       "      <td>[1, 2, 2, 2, 0, 0, 3, 4, 4, 4, 4, 0, 0, 0, 5, ...</td>\n",
       "      <td>오프커피에서아메리카노핫으로2잔주문해주세요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[롤, 키, 에, 서, 치, 즈, 롤, 케, 이, 크, 한, 조, 각, 이, 랑, ...</td>\n",
       "      <td>[1, 2, 0, 0, 3, 4, 4, 4, 4, 4, 5, 6, 6, 0, 0, ...</td>\n",
       "      <td>롤키에서치즈롤케이크한조각이랑아메리카노아이스로하나씩부탁드립니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[리, 스, 아, 라, 비, 카, 에, 서, 라, 떼, 에, 디, 카, 페, 인, ...</td>\n",
       "      <td>[1, 2, 2, 2, 2, 2, 0, 0, 3, 4, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>리스아라비카에서라떼에디카페인옵션으로두잔,라지사이즈로부탁드릴게요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[C, O, F, F, E, E, R, O, M, A, N, 에, 서, 플, 랫, ...</td>\n",
       "      <td>[1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 3, 4, ...</td>\n",
       "      <td>COFFEEROMAN에서플랫화이트두잔,소이라떼하나로총3잔주문할게요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[고, 더, 커, 피, 에, 서, 카, 푸, 치, 노, 2, 잔, ,, 시, 나, ...</td>\n",
       "      <td>[1, 2, 2, 2, 0, 0, 3, 4, 4, 4, 5, 6, 0, 0, 0, ...</td>\n",
       "      <td>고더커피에서카푸치노2잔,시나몬추가로부탁드립니다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [오, 프, 커, 피, 에, 서, 아, 메, 리, 카, 노, 핫, 으, 로, 2, ...   \n",
       "1  [롤, 키, 에, 서, 치, 즈, 롤, 케, 이, 크, 한, 조, 각, 이, 랑, ...   \n",
       "2  [리, 스, 아, 라, 비, 카, 에, 서, 라, 떼, 에, 디, 카, 페, 인, ...   \n",
       "3  [C, O, F, F, E, E, R, O, M, A, N, 에, 서, 플, 랫, ...   \n",
       "4  [고, 더, 커, 피, 에, 서, 카, 푸, 치, 노, 2, 잔, ,, 시, 나, ...   \n",
       "\n",
       "                                            ner_tags  \\\n",
       "0  [1, 2, 2, 2, 0, 0, 3, 4, 4, 4, 4, 0, 0, 0, 5, ...   \n",
       "1  [1, 2, 0, 0, 3, 4, 4, 4, 4, 4, 5, 6, 6, 0, 0, ...   \n",
       "2  [1, 2, 2, 2, 2, 2, 0, 0, 3, 4, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 3, 4, ...   \n",
       "4  [1, 2, 2, 2, 0, 0, 3, 4, 4, 4, 5, 6, 0, 0, 0, ...   \n",
       "\n",
       "                               sentence  \n",
       "0                오프커피에서아메리카노핫으로2잔주문해주세요  \n",
       "1     롤키에서치즈롤케이크한조각이랑아메리카노아이스로하나씩부탁드립니다  \n",
       "2    리스아라비카에서라떼에디카페인옵션으로두잔,라지사이즈로부탁드릴게요  \n",
       "3  COFFEEROMAN에서플랫화이트두잔,소이라떼하나로총3잔주문할게요  \n",
       "4             고더커피에서카푸치노2잔,시나몬추가로부탁드립니다  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER_TAG 맵핑\n",
    "# - 매장명(STR), 메뉴(N), 수량(CNT)\n",
    "id2label = {1: \"B-STR\", 2: \"I-STR\", 3: \"B-N\", 4: \"I-N\", 5: \"B-CNT\", 6: \"I-CNT\", 0: \"O\"}\n",
    "label2id = {\"B-STR\": 1, \"I-STR\": 2, \"B-N\": 3, \"I-N\": 4, \"B-CNT\": 5, \"I-CNT\": 6, \"O\": 0}\n",
    "\n",
    "# 맵핑 적용\n",
    "tagDF[\"ner_tags\"] = tagDF[\"ner_tags\"].apply(\n",
    "    lambda tags: [id2label[int(tag)] for tag in tags]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 812 entries, 0 to 811\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   tokens    812 non-null    object\n",
      " 1   ner_tags  812 non-null    object\n",
      " 2   sentence  812 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 19.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "                                               tokens  \\\n",
       " 0  [오, 프, 커, 피, 에, 서, 아, 메, 리, 카, 노, 핫, 으, 로, 2, ...   \n",
       " 1  [롤, 키, 에, 서, 치, 즈, 롤, 케, 이, 크, 한, 조, 각, 이, 랑, ...   \n",
       " 2  [리, 스, 아, 라, 비, 카, 에, 서, 라, 떼, 에, 디, 카, 페, 인, ...   \n",
       " \n",
       "                                             ner_tags  \\\n",
       " 0  [B-STR, I-STR, I-STR, I-STR, O, O, B-N, I-N, I...   \n",
       " 1  [B-STR, I-STR, O, O, B-N, I-N, I-N, I-N, I-N, ...   \n",
       " 2  [B-STR, I-STR, I-STR, I-STR, I-STR, I-STR, O, ...   \n",
       " \n",
       "                              sentence  \n",
       " 0              오프커피에서아메리카노핫으로2잔주문해주세요  \n",
       " 1   롤키에서치즈롤케이크한조각이랑아메리카노아이스로하나씩부탁드립니다  \n",
       " 2  리스아라비카에서라떼에디카페인옵션으로두잔,라지사이즈로부탁드릴게요  )"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagDF.info(), tagDF.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(649, 2) (649,) (163, 2) (163,)\n"
     ]
    }
   ],
   "source": [
    "# Datasets  형태로 전환\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 학습용, 검증용, 테스트용 데이터 분리\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    tagDF.drop([\"ner_tags\"], axis=1), tagDF[\"ner_tags\"], test_size=0.2, random_state=42\n",
    ")\n",
    "test_df=pd.read_csv('./testDF.csv')\n",
    "x_test, y_test= testingDF['tokens'], testingDF['ner_tags']\n",
    "print(x_train.shape, y_train.shape, x_valid.shape, y_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(testingDF['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스 형태로 맞추기\n",
    "\n",
    "train_data = {\n",
    "    \"sentence\": x_train['sentence'],\n",
    "    \"tokens\": x_train[\"tokens\"],\n",
    "    \"ner_tags\": y_train,\n",
    "}\n",
    "trainDS = Dataset.from_dict(train_data)\n",
    "valid_data = {\n",
    "    \"sentence\": x_valid['sentence'],\n",
    "    \"tokens\": x_valid[\"tokens\"],\n",
    "    \"ner_tags\": y_valid,\n",
    "}\n",
    "validDS = Dataset.from_dict(valid_data)\n",
    "test_data= {\n",
    "    'tokens': x_test,\n",
    "    'ner_tags': y_test\n",
    "}\n",
    "testDF= pd.read_csv('./test_data.csv')\n",
    "testDS= Dataset.from_pandas(testDF)\n",
    "dataset = DatasetDict({\"train\": trainDS, \"valid\": validDS,\n",
    "                       'test': testDS})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'tokens', 'ner_tags'],\n",
       "        num_rows: 649\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['sentence', 'tokens', 'ner_tags'],\n",
       "        num_rows: 163\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 79\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset\n",
    "# 데이터셋의 형태\n",
    "# dataset (dict 형태)\n",
    "# - train  (list 형태-> 안의 요소가 dict로 구성)\n",
    "# - sentence\n",
    "# - toekns\n",
    "# - labels\n",
    "\n",
    "# - test    (list 형태)\n",
    "# - sentence\n",
    "# - toekns\n",
    "# - labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = list(id2label.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], padding=True, padding_side='right', is_split_into_words=True\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                try:\n",
    "                    if label[word_idx] == \"O\":\n",
    "                        label_ids.append(0)\n",
    "                    else:\n",
    "                        label_ids.append(int(label2id[label[word_idx]]))\n",
    "                except ValueError:\n",
    "                    label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_wnut = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 학습 파라미터 지정<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# seqeval -> 분류에 대한 여러 평가지표 산출\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "labels = list(id2label.values())\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "\n",
    "    # ['O' 레이블 가중치]*0.5 -> 'O' 레이블로만 예측되는 문제 개선(클래스 불균형 문제)\n",
    "    for i in range(len(predictions)):\n",
    "        for j in range(len(predictions[i])):\n",
    "            predictions[i][j][0] = predictions[i][j][0] * 0.5\n",
    "\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설정\n",
    "from transformers import (AutoModelForTokenClassification, \n",
    "                          LlamaForTokenClassification, TrainingArguments, \n",
    "                          Trainer, EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "\n",
    "model = LlamaForTokenClassification.from_pretrained(\n",
    "    model_name, num_labels=7, id2label=id2label, label2id=label2id\n",
    ")\n",
    "# , num_labels=7, id2label=id2label, label2id=label2id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]c:\\Users\\KDT\\anaconda3\\envs\\llama_38\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "                                                \n",
      " 10%|█         | 20/200 [00:08<00:33,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7833276391029358, 'eval_precision': 0.6720080120180271, 'eval_recall': 0.7530864197530864, 'eval_f1': 0.7102408044456204, 'eval_accuracy': 0.7107843137254902, 'eval_runtime': 0.5155, 'eval_samples_per_second': 306.489, 'eval_steps_per_second': 9.699, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 20%|██        | 40/200 [06:29<00:49,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.269212543964386, 'eval_precision': 0.9416642836717186, 'eval_recall': 0.9239618406285073, 'eval_f1': 0.9327290752018127, 'eval_accuracy': 0.9392156862745098, 'eval_runtime': 0.4383, 'eval_samples_per_second': 360.509, 'eval_steps_per_second': 11.409, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 30%|███       | 60/200 [12:45<00:40,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2016952633857727, 'eval_precision': 0.9450891593546561, 'eval_recall': 0.9368686868686869, 'eval_f1': 0.9409609694237002, 'eval_accuracy': 0.9487745098039215, 'eval_runtime': 0.3795, 'eval_samples_per_second': 416.373, 'eval_steps_per_second': 13.176, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 40%|████      | 80/200 [19:07<00:37,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1891062706708908, 'eval_precision': 0.9493670886075949, 'eval_recall': 0.946969696969697, 'eval_f1': 0.9481668773704172, 'eval_accuracy': 0.9529411764705882, 'eval_runtime': 0.4764, 'eval_samples_per_second': 331.65, 'eval_steps_per_second': 10.495, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 50%|█████     | 100/200 [25:24<00:30,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1843980997800827, 'eval_precision': 0.9585331452750353, 'eval_recall': 0.9534231200897868, 'eval_f1': 0.9559713039808694, 'eval_accuracy': 0.9600490196078432, 'eval_runtime': 0.4258, 'eval_samples_per_second': 371.072, 'eval_steps_per_second': 11.743, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 60%|██████    | 120/200 [31:41<00:23,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19100601971149445, 'eval_precision': 0.9544811987560079, 'eval_recall': 0.9472502805836139, 'eval_f1': 0.9508519926770876, 'eval_accuracy': 0.9575980392156863, 'eval_runtime': 0.362, 'eval_samples_per_second': 436.512, 'eval_steps_per_second': 13.814, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 70%|███████   | 140/200 [38:07<00:18,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18793492019176483, 'eval_precision': 0.9591664319909885, 'eval_recall': 0.9556677890011224, 'eval_f1': 0.957413914265636, 'eval_accuracy': 0.961764705882353, 'eval_runtime': 0.4167, 'eval_samples_per_second': 379.188, 'eval_steps_per_second': 12.0, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 80%|████████  | 160/200 [44:28<00:12,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1961502581834793, 'eval_precision': 0.9616577389343107, 'eval_recall': 0.9570707070707071, 'eval_f1': 0.9593587399803122, 'eval_accuracy': 0.9634803921568628, 'eval_runtime': 0.3882, 'eval_samples_per_second': 406.96, 'eval_steps_per_second': 12.878, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 160/200 [50:46<12:41, 19.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3046.2766, 'train_samples_per_second': 2.075, 'train_steps_per_second': 0.066, 'train_loss': 0.5897091865539551, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=160, training_loss=0.5897091865539551, metrics={'train_runtime': 3046.2766, 'train_samples_per_second': 2.075, 'train_steps_per_second': 0.066, 'total_flos': 1224173570120880.0, 'train_loss': 0.5897091865539551, 'epoch': 8.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 하이퍼 파라미터 설정\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama_result_1\",\n",
    "    learning_rate=1e-4,\n",
    "    # gradient_accumulation_steps=2,\n",
    "    fp16=True,\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    dataloader_pin_memory=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_wnut[\"train\"],\n",
    "    eval_dataset=tokenized_wnut[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback],\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 데이터 평가\n",
    "trainer_test = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_wnut[\"train\"],\n",
    "    eval_dataset=tokenized_wnut[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback],\n",
    ")\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.40524781341107874\n",
      "f1_score : 0.3914151782498473\n",
      "mse : 5.41649312786339\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.29      0.34       585\n",
      "           1       0.00      0.00      0.00        82\n",
      "           2       0.58      0.72      0.64       389\n",
      "           3       0.02      0.02      0.02       165\n",
      "           4       0.50      0.57      0.53       854\n",
      "           5       0.08      0.08      0.08       152\n",
      "           6       0.11      0.10      0.10       174\n",
      "\n",
      "    accuracy                           0.41      2401\n",
      "   macro avg       0.24      0.25      0.25      2401\n",
      "weighted avg       0.39      0.41      0.39      2401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "from sklearn.metrics import *\n",
    "import pandas as pd\n",
    "import os, json\n",
    "\n",
    "\n",
    "DEVICE='cuda'\n",
    "test_dir= os.listdir('./character_test')\n",
    "for test_file in test_dir:\n",
    "    if test_dir.index(test_file)==0:\n",
    "        testDF=pd.read_json(f'./character_test/{test_file}', encoding='utf-8')\n",
    "    else:\n",
    "        testDF2=pd.read_json(f'./character_test/{test_file}', encoding='utf-8')\n",
    "        testDF=pd.concat([testDF, testDF2])\n",
    "tokens = testDF['tokens']\n",
    "ner_tags = testDF['ner_tags']\n",
    "\n",
    "\n",
    "\n",
    "total_pred_list = []\n",
    "total_target_list = []\n",
    "for token, ner_tag in zip(tokens, ner_tags):\n",
    "    tokenized_inputs = tokenizer(token, padding=True, padding_side='right' , is_split_into_words=True, return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(\n",
    "            input_ids = tokenized_inputs['input_ids'].to(DEVICE),\n",
    "            attention_mask = tokenized_inputs['attention_mask'].to(DEVICE)\n",
    "        )\n",
    "    pred=output.logits\n",
    "    pred_list = pred.argmax(axis=2).tolist()\n",
    "    total_pred_list.extend(pred_list[0][:len(token)])\n",
    "    total_target_list.extend([int(a) for a in ner_tag])\n",
    "\n",
    "try:\n",
    "    accuracy = accuracy_score(total_target_list, total_pred_list)\n",
    "    f1 = f1_score(total_target_list, total_pred_list, average='weighted')\n",
    "    mse = mean_squared_error(total_target_list, total_pred_list)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print(f\"accuracy : {accuracy}\")\n",
    "print(f\"f1_score : {f1}\")\n",
    "print(f\"mse : {mse}\")\n",
    "print(classification_report(total_target_list, total_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./result_2\\\\tokenizer_config.json',\n",
       " './result_2\\\\special_tokens_map.json',\n",
       " './result_2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./result_2\")\n",
    "tokenizer.save_pretrained(\"./result_2\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal_projet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
