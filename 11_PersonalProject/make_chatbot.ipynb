{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 세금 관련 챗봇 모델 생성<hr>\n",
    "1. 데이터 전처리 (분류 모델을 통해)\n",
    "2. 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터 전처리<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#패키지 불러오기\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# from sentence_split import split_answer\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "#패키지 불러오기\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "# from sentence_split import split_answer\n",
    "import os\n",
    "from kss import split_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 경로 폴더 지정\n",
    "DATA_PATH='./test_1.csv'\n",
    "dataDF=pd.read_csv(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_answer(DataFrame, save_name, save_path):\n",
    "\n",
    "dataDF=dataDF\n",
    "splited_answer=[]\n",
    "# 행마다 문장분리 실행\n",
    "for a in dataDF['answer']:\n",
    "    # a= a.replace('\\u200b', '')\n",
    "    a_splited=split_sentences(a)\n",
    "    # 구두점 넣기\n",
    "    sentence_list=[]\n",
    "    # print(a_splited, type(a_splited))\n",
    "    for sentence in a_splited:\n",
    "        sentence=str(sentence)\n",
    "        # print(sentence, type(sentence))\n",
    "        if sentence[-1] not in ['.', '!', '?', '~', ')', '}', ']',\n",
    "                                '&', '>']:\n",
    "            sentence_list.append(sentence+'.')\n",
    "            # print(sentence)\n",
    "        else:\n",
    "            sentence_list.append(sentence)\n",
    "            # print('구두점 존재')\n",
    "    # 분리한 행 재조합\n",
    "    \n",
    "    splited_answer.append(sentence_list)\n",
    "# 데이터 프레임으로 반환\n",
    "dataDF['answer']=splited_answer\n",
    "# splited로 이름 바꾸기\n",
    "# data=data.replace('after', 'splited')\n",
    "dataDF.to_csv('./test_splited.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "# 문장 분류기로 필요없는 문장 제거\n",
    "splitDF=pd.read_csv('./split_data.csv')\n",
    "#파이프라인 생성\n",
    "classifier = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=\"./model_CLF_2/\",\n",
    "    tokenizer=\"./model_CLF_2/\", \n",
    "    device='cuda'\n",
    ")\n",
    "# 문장마다 분류기 실행\n",
    "cleaned_text_lists=[]\n",
    "for row in splitDF['answer']:\n",
    "    row=[]\n",
    "    for sentence in row: \n",
    "        print(sentence)\n",
    "        results = classifier(sentence)\n",
    "        # 결과에 따른 전처리 처리\n",
    "        if result['label']== 'LABEL_0':\n",
    "            texts.remove(text)\n",
    "        else: row.append(text)\n",
    "    # 전처리한 행 하나의 string으로 재조합\n",
    "    cleaned_text_lists.append(row)\n",
    "splitDF['answer']=cleaned_text_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitDF=pd.read_csv('./split_data.csv')\n",
    "type(list(splitDF['answer'])[0])\n",
    "# for row in splitDF['answer']:\n",
    "#     new_row=[]\n",
    "#     print(row[1])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 클래스: 0\n",
      "클래스별 확률: [0.9609748721122742, 0.03902510553598404]\n"
     ]
    }
   ],
   "source": [
    "# 필요없는 문장 제거 하기\n",
    "import torch\n",
    "from transformers import T5ForSequenceClassification, T5Tokenizer\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# 분류 모델 지정하기\n",
    "MODEL_NAME='./model_CLF_2'\n",
    "tokenizer=T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model= T5ForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(DEVICE)\n",
    "\n",
    "\n",
    "# 예측하기 (테스트)\n",
    "def predict_text(text):\n",
    "    text=text\n",
    "    inputs = tokenizer(f\"classify: {text}\", return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs.to(DEVICE))\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predicted_class = predictions.argmax().item()\n",
    "    return predicted_class, predictions[0].tolist()\n",
    "\n",
    "text = \"정말 좋은 영화였어요\"\n",
    "predicted_class, probabilities = predict_text(text)\n",
    "print(f\"예측 클래스: {predicted_class}\")\n",
    "print(f\"클래스별 확률: {probabilities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 문류를 통한 데이터 전처리 수행\n",
    "import re\n",
    "new_rows=[]\n",
    "for row in splitDF['answer']:\n",
    "    # 콤마 기준으로 문장 분리\n",
    "    sentence_list = re.split(r'[,]', row)\n",
    "    for sentence in sentence_list:\n",
    "        # 문장당 분류 모델 실행\n",
    "        result, _=predict_text(sentence)\n",
    "        if result==0:\n",
    "            sentence_list.remove(sentence)\n",
    "        else: pass\n",
    "    new_rows.append(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 열에 적용\n",
    "splitDF['answer']=new_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>제가 일반사업자 및 직장인인데 이번에 빌라를 팔면 소득이 생길거같은데 소득세와종합소...</td>\n",
       "      <td>[ '저의 답변이 도움이 되기를 바랍니다.',  '1. 일반사업자 및 직장인인데 빌...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>종합소득세는 개인이 1년간 벌어들인 소득에 대해 부과되는 세금으로, 과세표준에 따라...</td>\n",
       "      <td>[['종합소득세율은 다음과 같습니다.',  '국세청 사이트https://www.nt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>종합소득세는 개인이 1년간 벌어들인 소득에 대해 부과되는 세금으로, 과세표준에 따라...</td>\n",
       "      <td>[ '종합소득세 세율은 누진세율(6%~45%)이 적용되며,  \"과세표준에 따라 적용...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>종합소득세는 개인이 1년간 벌어들인 소득에 대해 부과되는 세금으로, 과세표준에 따라...</td>\n",
       "      <td>[ '5월에는 종합소득세 납부 기간으로 사업하시는 분들이나 프리...blog.nav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>종합소득세는 개인이 1년간 벌어들인 소득에 대해 부과되는 세금으로, 과세표준에 따라...</td>\n",
       "      <td>[ '아래 링크에서 볼 수 있습니다.',  'https://www.nts.go.kr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           question  \\\n",
       "0           0  제가 일반사업자 및 직장인인데 이번에 빌라를 팔면 소득이 생길거같은데 소득세와종합소...   \n",
       "1           1  종합소득세는 개인이 1년간 벌어들인 소득에 대해 부과되는 세금으로, 과세표준에 따라...   \n",
       "2           2  종합소득세는 개인이 1년간 벌어들인 소득에 대해 부과되는 세금으로, 과세표준에 따라...   \n",
       "3           3  종합소득세는 개인이 1년간 벌어들인 소득에 대해 부과되는 세금으로, 과세표준에 따라...   \n",
       "4           4  종합소득세는 개인이 1년간 벌어들인 소득에 대해 부과되는 세금으로, 과세표준에 따라...   \n",
       "\n",
       "                                              answer  \n",
       "0  [ '저의 답변이 도움이 되기를 바랍니다.',  '1. 일반사업자 및 직장인인데 빌...  \n",
       "1  [['종합소득세율은 다음과 같습니다.',  '국세청 사이트https://www.nt...  \n",
       "2  [ '종합소득세 세율은 누진세율(6%~45%)이 적용되며,  \"과세표준에 따라 적용...  \n",
       "3  [ '5월에는 종합소득세 납부 기간으로 사업하시는 분들이나 프리...blog.nav...  \n",
       "4  [ '아래 링크에서 볼 수 있습니다.',  'https://www.nts.go.kr...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 데이터셋 형성<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5028 942 316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5028, 942, 316)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 분리\n",
    "\n",
    "textDF=splitDF[['question', 'answer']].reset_index(drop=True)\n",
    "train_length= int(len(textDF)*0.8)\n",
    "valid_length=int(len(textDF)*0.15)\n",
    "test_length=int(len(textDF)-train_length-valid_length)\n",
    "print(train_length, valid_length, test_length)\n",
    "trainDF=textDF[:train_length]\n",
    "validDF=textDF.drop(trainDF.index).reset_index(drop=True)[:valid_length]\n",
    "testDF=textDF.drop(trainDF.index).reset_index(drop=True)[valid_length:]\n",
    "len(trainDF), len(validDF), len(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "eval() arg 1 must be a string, bytes or code object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m new_rows\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m trainDF[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m----> 3\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m     new_rows\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[0;32m      5\u001b[0m trainDF[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mnew_rows\n",
      "\u001b[1;31mTypeError\u001b[0m: eval() arg 1 must be a string, bytes or code object"
     ]
    }
   ],
   "source": [
    "new_rows=[]\n",
    "for row in trainDF['answer']:\n",
    "    result = '\\n'.join(eval(row))\n",
    "    new_rows.append(result)\n",
    "trainDF['answer']=new_rows\n",
    "new_rows=[]\n",
    "for row in validDF['answer']:\n",
    "    result = '\\n'.join(eval(row))\n",
    "    new_rows.append(result)\n",
    "validDF['answer']=new_rows\n",
    "new_rows=[]\n",
    "for row in testDF['answer']:\n",
    "    result = '\\n'.join(eval(row))\n",
    "    new_rows.append(result)\n",
    "testDF['answer']=new_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "trainDF.to_csv('./train_3.csv', index=False)\n",
    "validDF.to_csv('./valid_3.csv', index=False)\n",
    "testDF.to_csv('./test_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dicts= {\"train\": \"train_3.csv\", \"valid\": \"valid_3.csv\", \"test\": \"test_3.csv\"}\n",
    "# 데이터셋 생성\n",
    "dataset= load_dataset('csv' ,data_files=data_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "# 사전학습된 모델 불러오기\n",
    "BASE='KETI-AIR/ke-t5-base-ko'\n",
    "tokenizer=T5TokenizerFast.from_pretrained(BASE)\n",
    "model= T5ForConditionalGeneration.from_pretrained(BASE)\n",
    "# 쿠다 설정\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model=model.to(device)\n",
    "# 문장 최대 길이 설정\n",
    "max_length=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 함수\n",
    "def make_tokenized(test):\n",
    "    tokenized= tokenizer(test['question'],\n",
    "                         text_target=test['answer'],\n",
    "                         max_length=200,\n",
    "                         truncation=True)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69048819b0d042b2a637f68689b8bc1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/316 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 토큰화 적용\n",
    "tokenizedDS=dataset.map(make_tokenized,\n",
    "batched=True,\n",
    "remove_columns=dataset['train'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5028\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 942\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 316\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizedDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원 데이터    : 저는 직장을 다니고 있고 저희 어머니께서 제 명의로 사업자를 내셔서 일반음식점 자영업을 하고 계십니다.이경우 발생하는 종합소득세는 사업자에서 발생한 세금일지 아니면 직장소득과 사업소득이 합쳐져 생긴 세금일지 궁금합니다.직장에서 연말정산 등등 모든 세금처리를 다하는데 종합소득세는 정확히 무엇인지가 궁금해요.\n",
      "처리 후 데이터: [1928, 31404, 33541, 561, 6460, 4663, 6225, 190, 21835, 4746, 21, 182, 24932, 1039, 41650, 38176, 11, 239, 55996, 3, 15, 508, 363, 5127, 1902, 33642, 12, 4746, 37, 2303, 6572, 30, 76, 3533, 5660, 5130, 31, 546, 5130, 15, 9776, 1781, 6321, 6572, 30, 76, 17282, 1008, 3, 25508, 37, 38097, 27923, 350, 6572, 4582, 21, 299, 2143, 1902, 33642, 12, 6919, 9118, 22, 16271, 328, 3, 1]\n",
      "토큰화       : ['▁저는', '▁직장을', '▁다니고', '▁있고', '▁저희', '▁어머니', '께서', '▁제', '▁명의로', '▁사업자', '를', '▁내', '셔서', '▁일반', '음식점', '▁자영업', '을', '▁하고', '▁계십니다', '.', '이', '경', '우', '▁발생하는', '▁종합', '소득세', '는', '▁사업자', '에서', '▁발생한', '▁세금', '일', '지', '▁아니면', '▁직장', '소득', '과', '▁사업', '소득', '이', '▁합쳐', '져', '▁생긴', '▁세금', '일', '지', '▁궁금', '합니다', '.', '직장', '에서', '▁연말정산', '▁등등', '▁모든', '▁세금', '처리', '를', '▁다', '하는데', '▁종합', '소득세', '는', '▁정확히', '▁무엇인지', '가', '▁궁금해', '요', '.', '</s>']\n",
      "\n",
      "\n",
      "원 데이터    : [\" '이경우 발생하는 종합소득세는 사업자에서 발생한 세금일지 아니면 직장소득과 사업소득이 합쳐져 생긴 세금일지 궁금합니다.'\", \" '직장에서 연말정산 등등 모든 세금처리를 다하는데 종합소득세는 정확히 무엇인지가 궁금해요.'\", \" 종합소득세등 세금 문제는 처음 들으시면 복잡하고 어렵게 들립니다.'\", \" '그래서종합소득세에 대해 이해하기 쉽게 설명드리겠습니다.'\", \" '종합소득세의 개념: 종합소득세는 개인이 한 해 동안 얻은 모든 소득을 합산하여 신고하고 납부하는 세금입니다.'\", \" '여기에는 근로소득(직장에서 받는 월급)과 사업소득(자영업에서 발생하는 수익) 등이 포함됩니다.'\", \" '소득 합산: 귀하의 경우\", \" 직장에서의 근로소득과 어머니 명의로 운영되는 일반음식점에서 발생한 사업소득이 합쳐져 종합소득세가 계산됩니다.'\", \" 두 가지 소득이 모두 포함되어 종합소득세 신고를 하게 됩니다.'\", \" '연말정산과 종합소득세: 직장에서 연말정산을 통해 이미 원천징수된 세금이 있지만\", \" 종합소득세 신고는 그 외의 소득을 포함하여 최종적으로 세금을 계산하는 과정입니다.'\", \" '연말정산은 근로소득에 대한 세금 정산이고\", \" 종합소득세 신고는 모든 소득을 포함한 종합적인 세금 처리입니다.'\", \" '신고 방법: 매년 5월에 종합소득세 신고를 하게 되며\", \" 이때 직장에서의 소득과 사업소득을 모두 합산하여 신고합니다.'\", \" '이렇게 신고한 소득에 대해 세무서에서 세금을 계산하고\", \" 만약 세금이 과오납되었다면 환급받을 수 있는 기회도 있습니다.'\", '질문자님의 경우 직장에서의 근로소득과어머니의 사업소득이 합쳐져종합소득세가 부과되며', \" 연말정산은 근로소득에 대한 세금 정산이라는 점을 기억해 주시면 됩니다.'\", \" '어머님의 경우 종합소득세 대상자인지 궁금하시다면 아래에서 쉽게 확인할 수 있습니다.'\", \" '종합소득세 신고대상 확인신고대상 조회하기 나도 신고 대상자?'\", \" '종합소득세 신고 종합소득세 신고는 개인의 소득을 정확히 파악하고 세금을 올바르게 납부하기 위한...naver.me.']\"]\n",
      "토큰화       : ['▁[', '\"', \"▁'\", '이', '경', '우', '▁발생하는', '▁종합', '소득세', '는', '▁사업자', '에서', '▁발생한', '▁세금', '일', '지', '▁아니면', '▁직장', '소득', '과', '▁사업', '소득', '이', '▁합쳐', '져', '▁생긴', '▁세금', '일', '지', '▁궁금', '합니다', '.', \"'\", '\"', ',', '▁\"', \"▁'\", '직장', '에서', '▁연말정산', '▁등등', '▁모든', '▁세금', '처리', '를', '▁다', '하는데', '▁종합', '소득세', '는', '▁정확히', '▁무엇인지', '가', '▁궁금해', '요', '.', \"'\", '\"', ',', '▁\"', '▁종합', '소득세', '등', '▁세금', '▁문제는', '▁처음', '▁들', '으시', '면', '▁복잡하고', '▁어렵게', '▁들', '립니다', '.', \"'\", '\"', ',', '▁\"', \"▁'\", '그래서', '종합', '소득세', '에', '▁대해', '▁이해하기', '▁쉽게', '▁설명', '드리겠습니다', '.', \"'\", '\"', ',', '▁\"', \"▁'\", '종합', '소득세', '의', '▁개념', ':', '▁종합', '소득세', '는', '▁개인이', '▁한', '▁해', '▁동안', '▁얻은', '▁모든', '▁소득을', '▁합산', '하여', '▁신고', '하고', '▁납부', '하는', '▁세금', '입니다', '.', \"'\", '\"', ',', '▁\"', \"▁'\", '여기', '에는', '▁근로소득', '(', '직장', '에서', '▁받는', '▁월급', ')', '과', '▁사업', '소득', '(', '자', '영업', '에서', '▁발생하는', '▁수익', ')', '▁등이', '▁포함', '됩니다', '.', \"'\", '\"', ',', '▁\"', \"▁'\", '소득', '▁합산', ':', '▁귀', '하의', '▁경우', '\"', ',', '▁\"', '▁직장에서', '의', '▁근로소득', '과', '▁어머니', '▁명의로', '▁운영되는', '▁일반', '음식점', '에서', '▁발생한', '▁사업', '소득', '이', '▁합쳐', '져', '▁종합', '소득세', '가', '▁계산', '됩니다', '.', \"'\", '\"', ',', '▁\"', '▁두', '▁가지', '▁소득이', '▁모두', '▁포함되어', '▁종합', '소득세', '▁신고를', '▁하게', '▁됩니다', '.', \"'\", '\"', '</s>']\n",
      "처리 후 데이터: [373, 29, 33, 15, 508, 363, 5127, 1902, 33642, 12, 4746, 37, 2303, 6572, 30, 76, 3533, 5660, 5130, 31, 546, 5130, 15, 9776, 1781, 6321, 6572, 30, 76, 17282, 1008, 3, 17, 29, 4, 23, 33, 25508, 37, 38097, 27923, 350, 6572, 4582, 21, 299, 2143, 1902, 33642, 12, 6919, 9118, 22, 16271, 328, 3, 17, 29, 4, 23, 1902, 33642, 1227, 6572, 2019, 919, 1761, 29963, 163, 46705, 9624, 1761, 15099, 3, 17, 29, 4, 23, 33, 19443, 5359, 33642, 9, 151, 18854, 1509, 3007, 26722, 3, 17, 29, 4, 23, 33, 5359, 33642, 6, 7664, 103, 1902, 33642, 12, 11399, 59, 347, 395, 5744, 350, 31652, 21428, 479, 2087, 72, 7102, 74, 6572, 335, 3, 17, 29, 4, 23, 33, 18494, 202, 38609, 26, 25508, 37, 1417, 14458, 18, 31, 546, 5130, 26, 127, 9300, 37, 5127, 4497, 18, 410, 2684, 4100, 3, 17, 29, 4, 23, 33, 5130, 21428, 103, 3694, 18910, 220, 29, 4, 23, 40612, 6, 38609, 31, 4663, 21835, 30832, 1039, 41650, 37, 2303, 546, 5130, 15, 9776, 1781, 1902, 33642, 22, 4469, 4100, 3, 17, 29, 4, 23, 152, 802, 19325, 263, 28681, 1902, 33642, 9909, 2957, 3455, 3, 17, 29, 1]\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 확인\n",
    "print( '원 데이터    :', dataset['train'][10]['question'] )\n",
    "print( '처리 후 데이터:', tokenizedDS['train'][10]['input_ids'] )\n",
    "print( '토큰화       :', tokenizer.convert_ids_to_tokens(tokenizedDS['train'][10]['input_ids']) )\n",
    "\n",
    "print('\\n')\n",
    "print( '원 데이터    :', dataset['train'][10]['answer'] )\n",
    "print( '토큰화       :', tokenizer.convert_ids_to_tokens(tokenizedDS['train'][10]['labels']) )\n",
    "print( '처리 후 데이터:', tokenizedDS['train'][10]['labels'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>decoder target</th>\n",
       "      <td>▁1</td>\n",
       "      <td>.</td>\n",
       "      <td>3.3%</td>\n",
       "      <td>사업</td>\n",
       "      <td>소득</td>\n",
       "      <td>원</td>\n",
       "      <td>천</td>\n",
       "      <td>징</td>\n",
       "      <td>수</td>\n",
       "      <td>시</td>\n",
       "      <td>...</td>\n",
       "      <td>▁'</td>\n",
       "      <td>물</td>\n",
       "      <td>음</td>\n",
       "      <td>표</td>\n",
       "      <td>▁둘</td>\n",
       "      <td>다</td>\n",
       "      <td>▁안</td>\n",
       "      <td>됩니다</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decoder input</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>▁1</td>\n",
       "      <td>.</td>\n",
       "      <td>3.3%</td>\n",
       "      <td>사업</td>\n",
       "      <td>소득</td>\n",
       "      <td>원</td>\n",
       "      <td>천</td>\n",
       "      <td>징</td>\n",
       "      <td>수</td>\n",
       "      <td>...</td>\n",
       "      <td>,</td>\n",
       "      <td>▁'</td>\n",
       "      <td>물</td>\n",
       "      <td>음</td>\n",
       "      <td>표</td>\n",
       "      <td>▁둘</td>\n",
       "      <td>다</td>\n",
       "      <td>▁안</td>\n",
       "      <td>됩니다</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0   1     2     3   4   5  6  7  8  9   ...  51  52 53 54  \\\n",
       "decoder target     ▁1   .  3.3%    사업  소득   원  천  징  수  시  ...  ▁'   물  음  표   \n",
       "decoder input   <pad>  ▁1     .  3.3%  사업  소득  원  천  징  수  ...   ,  ▁'  물  음   \n",
       "\n",
       "                55  56  57   58   59    60  \n",
       "decoder target  ▁둘   다  ▁안  됩니다    .  </s>  \n",
       "decoder input    표  ▁둘   다   ▁안  됩니다     .  \n",
       "\n",
       "[2 rows x 61 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인코더 입력\n",
    "encoder_inputs = tokenizer(\n",
    "    [\"사장님이 간이사업자라 4대보험은 내년부터 되고원청징수 3.3프로 신청을 홈택스로 해주셨는데원래 제 개인정보가 안들어 가고 사장님 사업소득신고러 되나요? 아니면 제가 추가적으로 근로소득을 신청해야하나요?\"],\n",
    "    return_tensors=\"pt\"\n",
    ")['input_ids'].to(device)\n",
    "# 디코더 입력을 위해 넣고\n",
    "decoder_targets = tokenizer(\n",
    "    [\"1.3.3%사업소득원천징수시 주민등록번호가 키코드입니다. 신분증사본을 제출해야 사업주가  사업소득신고를 할 수 있습니다.2.사업소득신고는 사업자가 해야합니다. 귀하가 근로소득으로 신고할 수 없습니다.', '물음표 둘다 안됩니다.\"],\n",
    "    return_tensors=\"pt\"\n",
    ")['input_ids'].to(device)\n",
    "# 디코더 쪽으로 이동\n",
    "decoder_inputs = model._shift_right(decoder_targets)\n",
    "\n",
    "pd.DataFrame(\n",
    "    [\n",
    "        tokenizer.convert_ids_to_tokens(decoder_targets[0]),\n",
    "        tokenizer.convert_ids_to_tokens(decoder_inputs[0])\n",
    "    ],\n",
    "    index=('decoder target', 'decoder input')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 49, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(input_ids=encoder_inputs,\n",
    "                decoder_input_ids=decoder_inputs,\n",
    "                labels=decoder_targets)\n",
    "outputs['encoder_last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 콜레이터 사용\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "trainDL=tokenizedDS['train']\n",
    "\n",
    "collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, \n",
    "                                model=model,\n",
    "                                return_tensors=\"pt\"\n",
    "                                )\n",
    "batch = collator(\n",
    "    [tokenizedDS[\"train\"][i] for i in range(1, 3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "# BLEU 점수 측정\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # decode preds and labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # 토큰을 텍스트로 디코딩\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # BLEU 점수 계산\n",
    "    bleu_score = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "    return {\"bleu\": bleu_score[\"bleu\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'compute_metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2SeqTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchkpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict_with_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Wandb 로그 끄기\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'compute_metrics'"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"chkpt\",\n",
    "    learning_rate=0.0005,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_strategy=\"no\",\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    gradient_accumulation_steps=3,\n",
    "    report_to=\"none\" # Wandb 로그 끄기\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenizedDS[\"train\"],\n",
    "    eval_dataset=tokenizedDS[\"valid\"],\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    ")\n",
    "# trainer.train()\n",
    "# trainer.save_model(\"./results_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "# 평가지표 테스트 (rouge)\n",
    "eval=evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aukii\\anaconda3\\envs\\nlp_38\\lib\\site-packages\\transformers\\generation\\utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9443972a1ab4e489e463b812b4bbfc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OverflowError",
     "evalue": "can't convert negative int to unsigned",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aukii\\anaconda3\\envs\\nlp_38\\lib\\site-packages\\transformers\\trainer_seq2seq.py:180\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aukii\\anaconda3\\envs\\nlp_38\\lib\\site-packages\\transformers\\trainer.py:3868\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3865\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3867\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3868\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3869\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3871\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3872\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3875\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3876\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3878\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\aukii\\anaconda3\\envs\\nlp_38\\lib\\site-packages\\transformers\\trainer.py:4160\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4156\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[0;32m   4157\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[0;32m   4158\u001b[0m         )\n\u001b[0;32m   4159\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4160\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4162\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[41], line 6\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(eval_pred)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 토큰을 텍스트로 디코딩\u001b[39;00m\n\u001b[0;32m      5\u001b[0m decoded_preds \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(predictions, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m decoded_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# BLEU 점수 계산\u001b[39;00m\n\u001b[0;32m      9\u001b[0m bleu_score \u001b[38;5;241m=\u001b[39m bleu_metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mdecoded_preds, references\u001b[38;5;241m=\u001b[39mdecoded_labels)\n",
      "File \u001b[1;32mc:\\Users\\aukii\\anaconda3\\envs\\nlp_38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3959\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[1;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3935\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[0;32m   3936\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3937\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3940\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3941\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   3942\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3943\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   3944\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3957\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   3958\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3959\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m   3960\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[0;32m   3961\u001b[0m             seq,\n\u001b[0;32m   3962\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3963\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3964\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3965\u001b[0m         )\n\u001b[0;32m   3966\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[0;32m   3967\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\aukii\\anaconda3\\envs\\nlp_38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3960\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3935\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[0;32m   3936\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3937\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3940\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3941\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   3942\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3943\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   3944\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3957\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   3958\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3959\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m-> 3960\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3964\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3965\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3966\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[0;32m   3967\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\aukii\\anaconda3\\envs\\nlp_38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3999\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3996\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[0;32m   3997\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[1;32m-> 3999\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4003\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4004\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aukii\\anaconda3\\envs\\nlp_38\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:654\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    653\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[1;32m--> 654\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    657\u001b[0m     clean_up_tokenization_spaces\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[0;32m    660\u001b[0m )\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[1;31mOverflowError\u001b[0m: can't convert negative int to unsigned"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notion_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
